{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "By now we have seen the GCN and DeepWalk, which when given a graph gave us the embedding for each node in the graph. These embedding can then be used for other purposes like link prediction or classification. Lets move on to a slightly different problem. In this problem we need the embeddings for each node of a graph where new nodes are continously been added. A possible way to do this would be do rerun the entire model (GCN or DeepWalk) again on the new graph, but it is computationaly expensive. Today  we will be covering a new method which will allow us to get embedding for such graphs is a much easier way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The start\n",
    "In the (GCN or DeepWalk) model, the graph was fixed beforehand, lets say the 'Zachary karate club', some model was trained on it, and then we could make prediction regarding which person went to which part of the club after separation.\n",
    "![Zachary Karate Club](img/karate_club.png \"Karate Club\")\n",
    "\n",
    "In this problem the nodes in this graph were fixed from beginning and all the predictions were also to be made on this fixed nodes. In contrast to this, take 'Youtube' videos to be the nodes and assume there is a edge between related videos, and say we have to classify these videos into categories depending on the content. If we take the same model as in the previous dataset, we can classify all these videos, but lets say some new video is added to 'YouTube', if we want to classify it we will have to retrain the model on the entire new dataset again. This is not feasible as there are too many videos for us to retrain.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To solve this issue, what we can do is to not learn embedding for each node but to learn a function which, given the features and edges joining this node, will give the embeddings for the node. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aggregating Neighbours\n",
    "\n",
    "The idea is to generate embeddings based on the neighbourhood of the given node. In other words, the embedding of a node will depend upon the embedding of the nodes it is connected to. Like in the graph below, the node 1 and 2 are likely to be more similar than node 1 and 5.\n",
    "![Simple_graph](img/example_graph_1.png \"Simple Graph\")\n",
    "\n",
    "Now lets see a simple method to generate embedding depending on neighbours.\n",
    "\n",
    "First we assign random values to the embeddings and on each step we will make the value of the embedding the average of the embeddings of all the nodes it shares an edge with. The following example shows the working on a simple linear graph.\n",
    "\n",
    "![Mean_Embeddings](img/animation.gif \"Mean Embeddings\")\n",
    "\n",
    "This is a very simple idea, which can be generalized by representing it in the following way\n",
    "![Simple_Neighbours](img/simple_neighbours.png \"Simple Neighbours\")\n",
    "\n",
    "Here The Black Box joining A with B, C , D represents some function of the A,B, C , D. ( In the above animation it was the mean function). We can replace this box by any function like say sum or max. This function is known as the aggregator function.\n",
    "\n",
    "Now lets try to make it more general by using not only the neighbhours of a node but also the neighbours of neighbours. The first question is how to make use of neighbours of neighbours. The way which we will be using here is to first generate each node's embedding in the first step by using only its neighbours just like we did above, then in the second step we will use these embeddings to generate the new embeddings. Take a look at the following \n",
    "\n",
    "![One_Layer_Aggregation](img/aggregation_1.png \"Aggregation Demo\")\n",
    "\n",
    "The numbers written along with the nodes are the value of embedding at time, T=0.\n",
    "\n",
    "Values of embedding after one step are as follows\n",
    "\n",
    "![Animation_aggregation_layer_1](img/animation_2.gif \"Aggregation Layer 1\")\n",
    "\n",
    "So after one interation the values are as follows:\n",
    "\n",
    "![Second_Layer_Aggregation](img/aggregation_2.png \"Aggregation After One Layer\")\n",
    "\n",
    "Repeating the same procedure again on this new graph we get (try verifying yourself)\n",
    "\n",
    "![Third_Layer_Aggregation](img/aggregation_3.png \"Aggregation After Two Layer\")\n",
    "\n",
    "Lets try to do some analysis of the aggregation. Represent by $A^{(0)}$ the initial value of embedding of A(i.e. 0.1), by $A^{(1)}$ the value after one layer(i.e. 0.25) similarly $A^{(2)}$, $B^{(0)}$, $B^{(1)}$ and all other values.\n",
    "\n",
    "Clearly \n",
    "\n",
    "$$A^{(1)} = \\frac{(A^{(0)} + B^{(0)} + C^{(0)} + D^{(0)})}{4}$$\n",
    "\n",
    "Similarly\n",
    "\n",
    "$$A^{(2)} = \\frac{(A^{(1)} + B^{(1)} + C^{(1)} + D^{(1)})}{4}$$\n",
    "\n",
    "Writing all the value in the RHS in terms of initial values of embeddings we get\n",
    "\n",
    "$$A^{(2)} = \\frac{\\frac{(A^{(0)} + B^{(0)} + C^{(0)} + D^{(0)})}{4} + \\frac{A^{(0)}+B^{(0)}+C^{(0)}}{3} + \\frac{A^{(0)}+B^{(0)}+C^{(0)}+E^{(0)} +F^{(0)}}{5} + \\frac{A^{(0)}+D^{(0)}}{2}}{4}$$\n",
    "\n",
    "If you look closely you will see the all the nodes that were either neighbours of A or neighbours of neighbours of A are present in this term. It is equavalent to saying that all nodes that have distance of less than or equal to 2 edges from A are influencing this term. Had there been a node G connected only to node F. then it is clearly at a distance of 3 from A and hence wont be influencing this term.\n",
    "\n",
    "Generalizing this we can say that if we repeat this produce N times, then all the nodes ( and only those nodes) that are at a within a distance N from the node will be influencing the value of the terms.\n",
    "\n",
    "If we replace the mean function, with some other function lets say $F$, then in this case we can write\n",
    "\n",
    "$$A^{(1)} = F(A^{(0)} , B^{(0)} , C^{(0)} , D^{(0)})$$\n",
    "\n",
    "Or more generally\n",
    "\n",
    "$$A^{(k)} = F(A^{(k-1)} , B^{(k-1)} , C^{(k-1)} , D^{(k-1)})$$\n",
    "\n",
    "If we denote by $N(v)$ the set of neighbours of $v$, so $N(A)=\\{B, C, D\\}$ and $N(A)^{(k)}=\\{B^{(k)}, C^{(k)}, D^{(k)}\\}$, the above equation can be simplified as\n",
    "\n",
    "$$A^{(k)} = F(A^{(k-1)}, N(A)^{(k-1)} )$$\n",
    "\n",
    "This process can be visualized as:\n",
    "\n",
    "![Sampling](img/showing_1.png \"Showing one\")\n",
    "\n",
    "\n",
    "This method is quite effective in generating the node embeddings. But there is an issue, if a new node is added to the graph how can be get its embeddings? This is an issue that cannot be tackled with this type of model. Clearly something new is needed, but what? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "One alternative that we can try is to replace the function F by multiple functions such that in first layer it is \n",
    "F1, in second layer F2 and so on, and then fixing the amount of layers that we want lets say k.\n",
    "\n",
    "So our embeddings will become like this\n",
    "\n",
    "\n",
    "![Sampling_2](img/showing_2.png \"Showing one\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets formalize our notation a bit now, so that it is easy to understand things.\n",
    "\n",
    "1. Instead of writing $A^{(k)}$  we will be writing $h_{A}^{k}$\n",
    "2. Rename the functions $F1$, $F2$ and so on as, $AGGREGATE_{1}$, $AGGREGATE_{2}$ and so on. i.e, $Fk$ becomes $AGGREGATE_{k}$.\n",
    "3. There will be a total of $K$ aggregation functions.\n",
    "3. Let our graph be represented by $G(V,E)$ where $V$ is the set of vertices and $E$ is the set of edges."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What GraphSAGE proposes?\n",
    "\n",
    "What we have been doing by now can be written as \n",
    "\n",
    "Initialise($h_{v}^{0}$) $\\forall v \\in V$ <br>\n",
    "for $k=1..K$ do <br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;for $v\\in V$ do<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$h_{v}^{k}=AGGREGATE_{k}(h_{v}^{k-1}, \\{h_{u}^{k-1} \\forall u \\in N(v)\\})$\n",
    "\n",
    "$h_{v}^{k}$ will now be containing the embeddings\n",
    "\n",
    "### Some issues with this\n",
    "\n",
    "Take a look at the sample graph that we discussed above, in this graph even though the initial embeddings for $E$ and $F$ were different but because their neighbours were same they ended with exactly the same embedding, this is not a a good things as their must be atleast some difference between their embeddings. \n",
    "\n",
    "GraphSAGE proposes an interesting idea to deal with it. Rather than passing both of the them into the same aggregating function, what we will do is to pass into aggregating function only the neighbours and then concantenating this vector with the vector of that node. This can be written as:\n",
    "\n",
    "$h_{v}^{k}=CONCAT(h_{v}^{k-1},AGGREGATE_{k}( \\{h_{u}^{k-1} \\forall u \\in N(v)\\}))$\n",
    "\n",
    "In this way we can prevent two vectors from attaining exactly the same embedding.\n",
    "\n",
    "Lets now add some non linearity to make it more expressive. So it becomes\n",
    "\n",
    "$h_{v}^{k}=\\sigma[W^{(k)}.CONCAT(h_{v}^{k-1},AGGREGATE_{k}( \\{h_{u}^{k-1} \\forall u \\in N(v)\\}))]$\n",
    "\n",
    "Where \\sigma is some non linear function (eg. RELU, sigmoid, etc.) and $W^{(k)}$ is the weight matrix, each layer will have one such matrix. If you looked closely, you will have seen that there no trainable parameters till now in our model. the $W$ matrix has been added to have something that model can learn to have some learning in the model.\n",
    "\n",
    "One more thing we will add is to normalize the value of h after each iteration,i.e, divide them by thier L2 norm, and hence our complete algorithm becomes.\n",
    "\n",
    "![GraphSAGE_Algorithm](img/graphsage_algorithm.png \"GraphSAGE\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the model learning, we need the loss function. For the general unsupervised learning problem the problem loss problem serves pretty well.\n",
    "\n",
    "![Loss](img/Loss_function.png \"Loss\")\n",
    "\n",
    "\n",
    "For supervised learning, either we can learn the embeddings first and then use those embeddings for the downstream task or combine both the part of learning embeddings and the part of applying these embeddings in the task into a single end to end models and then use the loss for the final part, and backpropagate to learn the embeddings while solving the task simultaneously.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aggregator Architectures\n",
    "One of the key difference between GCN and Graphsage is the generalisation of the aggregation function which was simply mean aggregation in GCN. So rather than simply taking the average we use generalised aggregation function in GraphSAGE. GraphSAGE owes its inductivity to its aggregator functions.\n",
    "\n",
    "## Mean aggregator \n",
    "Mean aggregator is as simple as you thought it would be. In mean aggregator we simply\n",
    "take the elementwise mean of the vectors in **{h<sub>u</sub><sup>k-1</sup>     ∀u ∈ N (v)}**.\n",
    "In other words, we can average embeddings of all nodes in the neighborhood to construct the neighborhood embedding.\n",
    "![mean aggregator](img/ma.png)\n",
    "\n",
    "## Pool aggregator\n",
    "Until now, we were using weighted average type of approach. But we could also use pooling type of approach for example we can do elementwise min or max pooling. So this would be another option where we are taking the messages from our neighbours, transforming them and applying some type of pooling technique(max pooling or min pooling).\n",
    "![pool aggregator](img/pa.png)\n",
    "\n",
    "In the above equation max denotes the elementwise max operator and σ is a nonlinear activation function (yes you are right it can be ReLU..). Please note that the function applied before the max pooling can be an arbitrarily deep multi-layer perceptron, but in the original paper simple single-layer architectures is preferred.\n",
    "\n",
    "## LSTM aggregator\n",
    "We could also use a deep neural network like LSTM to learn how to aggregate the neighbours. Since LSTM is not order invariant but we want our aggregator to be order invariant so when we train we could try to train this over several random orderings or permutation to make sure that this will learn that order is not important."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inductive capability\n",
    "One of the interesting propertery of GraphSAGE is that we can train our model on one subset of graph and apply this model  on other subset of this graph. The reason we can do this is that we can do parameter sharing i.e. those processing boxes are the same everywhere (W and B are shared across all the computational graphs or architectures). So when a new architecture comes into play we can borrow the parameters (W and B), do a forward pass and we get our prediction. \n",
    "![sharing parameters](img/sharing_param.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This property of GraphSAGE is very useful in the prediction of protein interaction. For example we can train our model on protein interaction graph from model organism A (left hand side in the figure below) and generate embedding on newly collected data from other model organism say B (right hand side in the figure).\n",
    "\n",
    "![protein_interaction](img/protein.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know that our old methods like deepwalk were not able to generalise to a new unseen graph. So if any new node gets added to the graph we had to train our model from scratch but since our new method is generalised to the unseen graphs, so in order to predict the embeddings of the new node we just have to make the computational graph of the new node, transfer the parameters to the unseen part of the graph and we can make predictions.\n",
    "\n",
    "![new node](img/new_node.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use this property in social-network (like facebook). Consider the first graph in the above figure, users in a social-network are represented by the nodes of the graph. Initially we have users represented by first graph, so we will train our model on this graph. After some time suppose another user is added in the network, now we don't have to train our model from scratch on the second graph, we will just create the computational graph of the new node, borrow the parameters from the already trained model and then we can find the embeddings of the newly added user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPORTANT : Please suggest some changes for the line commented using //...// in the markdown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeepWalk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hi, today I will be introducing you to a technique for converting nodes of a given graph to vector, something similar to what is done in the NLP, where words are converted to vector. These vector can be used to extract meaningful insight from the data. \n",
    "\n",
    "So lets begin.\n",
    "\n",
    "Lets assume that you have been given a graph G whose nodes you want to convert to vectors, the only information about a node is the indexes of the nodes to which it is connected. Since there is no initial feature matrix corresponding to the data, we will construct a feature matrix which will have all the randomly selected nodes. There can be multiple methods to select these but here we will be assuming that they are taken from the normal distribution ( though it won't make much difference if they are taken from some other distribution). \n",
    "\n",
    "//Since everything is selected out randomly hence this feature matrix will be useles in its presents form. But there is a technique to convert it into something very useful. Thats called the 'DeepWalk'.//\n",
    "\n",
    "## Intuition\n",
    "\n",
    "First lets get some intution regarding whats gonna happen. \n",
    "\n",
    "Lets deal with an unrelated problem first.\n",
    "\n",
    "Given some english sentences ( could be any other language, doesn't matter) you have to find a vector corresponding to each word appearing at least once in the sentence such that the words whose meaning is close to each other have their vector similar to each other also, the opposite must hold for the antonym.\n",
    "\n",
    "Suppose the sentences are\n",
    "1. Hi I am Sally.\n",
    "2. Hello this is Sally here.\n",
    "\n",
    "From the above sentences you can see that the 1 and 3 are pretty related with each other, so even if someone does not know english they can get a rough idea that the words Hi and Hello have roughly the same meaning. We will be using a technique similar to what a human being uses while trying to find out which words are related. Yup, We will be guessing the meaning based on the words which are common between the sentences. Now the question comes, how are we gonna do that.\n",
    "\n",
    "First lets assign a random vector to each word. Now since these vectors are assigned at random implies the representation in then present form is useless, so how are we gonna make this representation better. Here the good old probability is gonna help us. We will try to maximize the probability of the appearence of a word given the words that appear with it. To maximise the probability, we first need to find out the probability. Lets assume the probability is given by 'P(x | y)' where y is the set of words that appear in the same sentence in which x occurs. Remember we are only taking one sentence at a time, so here first we will try to maximise the probability of 'Hi' given {'I', 'am', 'Sally'} , then we will maximise the probability of 'I' given {'Hi', 'am', 'Sally'}. We will do it for each word in the first sentence , then for the second sentence. Repeating this procedure for all the sentence, and then we will repeat the procedure over all the sentences once again until the feature vector of the words have converged. One question that may arise now is,' How does these feature vectors related with probability?'. The answer is that in the probability function we will not be using the words directly we will actually be using the vectors assinged to them.But aren't those vectors random. Yup they are , but I promise you by the end of the blog they would have converged to values they really assings some meaning to those seamingly random numbers.\n",
    "\n",
    "First how this probability function works.\n",
    "\n",
    "What does it mean to find the probability of a vector given other vectors. This is actually a pretty question with a pretty simple answer, Take it as a fill up the blank problem that you dealt in the primary school, Fill up the blank in the given sentence\n",
    "'Roses ____ red.'. What is the  most likely guess? Most people will fill a 'are' in the blank space (Unless they are pretending to be oversmart in an attempt to prove how cool they are). \n",
    "\n",
    "The probability function is also trying to the same it is finding out the word which is most likely to occur given the word that are surrounding it. But it still doesn't tells us how it is gonna do that.\n",
    "\n",
    "How our brain solve this problem? Hint: It has to do with neurons.\n",
    "\n",
    "// In case you guessed 'Neural Network' you are correct. In this blog we will also be using neural networs (or a shitty copy of the human brain) ( I know this line was as shit as the neural nets are but I am feeling sleepy and cant think of a better line) ( Please suggest something better).//\n",
    "\n",
    "//Its not necesary to use the neural networks in the probability funciton but it looks cool so we will be using neural net here.//\n",
    "\n",
    "The input layer will have |V| neurons, where |V| is the number of words that are interesting to us. We will be using only one hidden layer for simplicity. It can have as many neurons as you want, but I suggest to keep is to be number that is less than the number of words in the vocabulary. The output layer will also have the |V| neurons.\n",
    "\n",
    "Now lets move on to in interpretation of what the input and output layer symbolises (don't care about the hidden layer).\n",
    "Lets suppose the words in the vocabulary are V1, V2, ... Vi, .... Vn. Assume that out of these V4,V7, V9 appears along with the word whose probability we are tying to maximise. so the input layers will have the 4th, 7th, and the 9th neuron with value 1 and all other will have the value 0. The hidden layer will then ve some function of these values. The hidden layer have no non linear acitvation. The |V| neuron in the output layer will have a score, the higher it is ,the higher the chances of that word appearning along with the surrounding words. Lets apply a sigmoid to convert these into probabilities. Eureka! we got the probabilities. \n",
    "\n",
    "\n",
    "So a simple neural network will help us fill in the blank problem.\n",
    "\n",
    "\n",
    "But what does it have to do with that so called 'DeepWalk'.\n",
    "Deepwalk works on graph so this separation based on sentences on graph. It uses a trick called random walk. It will start at the node whose probability we want to find out. Wait Wait Wait, we dont have the surrounding words , how it will find out the probability without them. The answer is that we will find then out in this step of Random Walk. Start at the node, find out all the nodes which have and edge connecting with this start node and randomly select on out of them , then consider this new node as the new start node and repeat the procedue after n iteration you will have traversed n nodes ( some of them might repeat, but it does not matter as in case of sentence words could have repeates). We will take n nodes as the surrounding nodes for the original node ans wil try to maximize probability with respect to those. So that is for you Ladies and Gentlemen , the 'DeepWalk' model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMPLEMENTING The DeepWalk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will use using a the following graph\n",
    "\n",
    "![title](graph.png)\n",
    "\n",
    "As you can see there are two connected components, so we can expect than when we create the vectors for each node, the vectors of [1 , 2, 3, 7] should be close and similarly that of [4, 5, 6] should be close. Also if  any two vectors are from different group then their vectors should also be far away."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will we representing the graph using the adjacency list representation. Make sure that you are able to understand that the given graph and this adjacency list are equivalent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj_list=[[1,2,3], [0,2,3], [0, 1, 3], [0, 1, 2], [5, 6], [4,6], [4, 5], [1, 3]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "w=3 #window size\n",
    "d=2 #embedding size\n",
    "y=200 #walks per vertex\n",
    "t=6 #walk length\n",
    "size_vertex=8 # no of vertices \n",
    "lr=0.025 # learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "v=[0,1,2,3,4,5,6,7] # labels of available vertices\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Walk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RandomWalk(v,t):\n",
    "    walk=[v]\n",
    "    for i in range(t-1):\n",
    "        v=adj_list[v][random.randint(0,len(adj_list[v])-1)]\n",
    "        walk.append(v)\n",
    "    return walk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define the skipgram model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Skipgram\n",
    "\n",
    "\n",
    "The skipgram model is closely related to the CBOW model that we just covered.In the CBOW model we have to maximise the probability of the word given its surrounding word using a neural network. And when the probability is maximised the weights of the input to hidden layer are the word vectors of the given words. In the skipgram word we will be using a using single word to maximise the probability of the surrounding words. This can be done by using a neural network that looks like the mirror image of the network that we used for the CBOW. And in the end the weights of the input to hidden layer will be the corresponding word vectors.\n",
    "\n",
    "\n",
    "Now lets analyze the complexity.\n",
    "There are |V| words in the vocabulary so for each iteration we will be modifying a total of |V| vectors. This is bad as usually the vocabulary size is in million and since we usually need millions of iteration before convergence, this can take a long time to run.\n",
    "\n",
    "We will soon be discussing some methods to reduce this complexity. But lets first look at the code for a simple skipgram model. The class defines the model , whereas the function 'SkipGram' takes care of all the training and other necessary stuff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class Model(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "  \n",
    "        super(Model, self).__init__()\n",
    "        self.phi=nn.Parameter(torch.rand((size_vertex, d), requires_grad=True))    \n",
    "        self.phi2=nn.Parameter(torch.rand((d, size_vertex), requires_grad=True))\n",
    "    def forward(self, one_hot):\n",
    "        hidden=torch.matmul(one_hot, self.phi)\n",
    "        out=torch.matmul(hidden,self.phi2)\n",
    "            \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=Model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SkipGram(wvi,  w):\n",
    "   \n",
    "    for j in range(len(wvi)):\n",
    "        for k in range(max(0,j-w) , min(j+w, len(wvi))):\n",
    "            #generate one hot vector\n",
    "            one_hot=torch.zeros( size_vertex)\n",
    "            one_hot[wvi[j]]=1\n",
    "            out=model(one_hot)\n",
    "            \n",
    "            loss=  - out[wvi[k]] + torch.log(torch.sum(torch.exp(out)))\n",
    "            loss.backward()\n",
    "            for param in model.parameters():\n",
    "                param.data.sub_(lr*param.grad)\n",
    "                param.grad.data.zero_()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(y):\n",
    "    random.shuffle(v)\n",
    "    for vi in v:\n",
    "        wvi=RandomWalk(vi,t)\n",
    "        SkipGram(wvi, w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "i'th row of the model.phi corresponds to vector of the i'th node. As you can see the vectors of [0, 1, 2,3 , 7] are very close, whereas their vector are much different from the vectors corresponding to [4, 5, 6]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.1972, -1.6208],\n",
      "        [ 0.6739, -1.3102],\n",
      "        [ 0.7101, -1.4672],\n",
      "        [ 0.0476, -1.5772],\n",
      "        [ 0.5641,  1.8627],\n",
      "        [ 0.9452,  1.7406],\n",
      "        [ 1.0284,  1.6569],\n",
      "        [ 0.1367, -0.3120]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(model.phi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above we defined and used the skipgram model. Now we will be discussing a variant known as the Hierarchical softmax."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hierarchical Softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have seen in the skip-gram model that the probability of any outcome depends on the total outcomes of model. If you haven't noticed, let me explain you how! Since when we calculate the probability of an outcomes using softmax, this probability depends on the number of model parameters via the normalisation constant(denominator term) in the softmax. And the number of such parameters is linear in the total number of outcomes. It means if we are dealing with a very large graphical structure, it can computationally expensive and can take a lot of time. Can we somehow overcome this challenge? Obviously yes!( because I'm asking at this stage). We can overcome this problem using \"hierarchical softmax\".Hierarchical softmax is an alternative to the softmax in which the probability of any one outcome depends on a number of model parameters that is only logarithmic in the total number of outcomes. \n",
    "\n",
    "Let's dive deep into Hierarchical softmax.\n",
    "\n",
    "Hierarchical softmax uses a binary tree to represent all words in the vocabulary. Each leaf of the tree is a node of our graph, and there is a unique path from root to leaf.Each intermediate node of tree explicitly represents the relative probabilities of its child nodes. So these nodes are associated to different certain vectors which our model is going to learn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea behind decomposing the output layer into binary tree is to reduce the time complexity to obtain \n",
    "probability distribution from O(V) to O(log(V))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us understand the process with an example.\n",
    "\n",
    "![binary tree](tree.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, leaf nodes represent the original nodes of our graph.The highlighted nodes and edges make a path from root to an example leaf node w2.\n",
    "\n",
    "Here length of the path L(w2) = 4.\n",
    "\n",
    "n(w, j) means the jth node on the path from root to a leaf node w."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now view this tree as a decision process, or a random walk, that begins at the root of the tree and descents towards the leaf nodes at each step. It turns out that the probability of each outcome in the original distribution uniquely determines the transition probabilities of this random walk.If you want to go from root node to w2(say), first you have to take a left turn, again left turn and then right turn. \n",
    "\n",
    "Let's denote the probability of going left at an intermediate node n as p(n,left) and probability of going right as p(n,right). So we can define the probabilty of going to w2 as follows.\n",
    "#### P(w2|wi) = p(n(w2,1),left) . p(n(w2,2),left) . p(n(w2,3),right) ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above process implies that the cost for computing the loss function and its gradient will be proportional to the number of nodes (V) in the intermediate path between root node and the output node, which on average is no greater than log (V). That's nice! Isn't it? In the case where we deal with a large number of outcomes, there will be a huge difference in the computational cost of 'vanilla' softmax and hierarchical softmax."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code is similar to the above part , except that we will only need to change to Model class by HierarchicalModel class, which is given below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#func_L returns the length of path from the root node to the given vertex\n",
    "\n",
    "def func_L(w):\n",
    "    cnt=1\n",
    "    while(w!=1):\n",
    "        cnt+=1\n",
    "        w//=2\n",
    "    return cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#func_n return the nth node in the path from the root node to the given vertex\n",
    "def func_n(w, j):\n",
    "    li=[w]\n",
    "    while(w!=1):\n",
    "        w=w//2;\n",
    "        li.append(w)\n",
    "    li.reverse()\n",
    "    return li[j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1/(1+torch.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HierarchicalModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "  \n",
    "        super(HierarchicalModel, self).__init__()\n",
    "        self.phi=nn.Parameter(torch.rand((size_vertex, d), requires_grad=True))   \n",
    "        self.prob_tensor=nn.Parameter(torch.rand((2*size_vertex, d), requires_grad=True))\n",
    "    def forward(self, wi, wo):\n",
    "        one_hot=torch.zeros( size_vertex)\n",
    "        one_hot[wi]=1\n",
    "        w=size_vertex+wo\n",
    "        h=torch.matmul(one_hot,self.phi)\n",
    "        p=torch.tensor([1.0])\n",
    "        for j in range(1,func_L(w)-1):\n",
    "            mult=-1\n",
    "            if(func_n(w, j+1)==2*func_n(w, j)): #Left child\n",
    "                mult=1\n",
    "            p=p*sigmoid(mult*torch.matmul(self.prob_tensor[func_n(w,j)], h))\n",
    "        \n",
    "        return p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input to hidden weight vector no longer represents the vector corresponding to each vector , so directly trying to read it will not provide any valuable insight, a better option is to predict the probability of different vectors against each other to figure out the likelihood of coexistance of the nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "hierarchicalModel=HierarchicalModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def HierarchicalSkipGram(wvi,  w):\n",
    "   \n",
    "    for j in range(len(wvi)):\n",
    "        for k in range(max(0,j-w) , min(j+w, len(wvi))):\n",
    "            #generate one hot vector\n",
    "       \n",
    "            prob=hierarchicalModel(wvi[j], wvi[k])\n",
    "            loss=  - torch.log(prob)\n",
    "            loss.backward()\n",
    "            for param in hierarchicalModel.parameters():\n",
    "                param.data.sub_(lr*param.grad)\n",
    "                param.grad.data.zero_()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(y):\n",
    "    random.shuffle(v)\n",
    "    for vi in v:\n",
    "        wvi=RandomWalk(vi,t)\n",
    "        HierarchicalSkipGram(wvi, w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31.0 30.0 14.0 22.0 19.0 9.0 3.0 67.0 \n",
      "25.0 34.0 15.0 23.0 16.0 8.0 0.0 75.0 \n",
      "19.0 28.0 25.0 26.0 20.0 20.0 1.0 56.0 \n",
      "23.0 33.0 18.0 24.0 17.0 10.0 0.0 71.0 \n",
      "38.0 16.0 23.0 22.0 33.0 33.0 33.0 0.0 \n",
      "27.0 18.0 29.0 23.0 28.0 38.0 32.0 0.0 \n",
      "32.0 19.0 23.0 23.0 30.0 30.0 38.0 0.0 \n",
      "22.0 37.0 15.0 24.0 13.0 6.0 0.0 80.0 \n"
     ]
    }
   ],
   "source": [
    "for i in range(8):\n",
    "    for j in range(8):\n",
    "        print((hierarchicalModel(i,j).item()*100)//1, end=' ')\n",
    "    print(end='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aggregator Architectures\n",
    "One of the key difference between GCN and Graphsage is the generalisation of the aggregation function which was simply mean aggregation in GCN. So rather than simply taking the average we use generalised aggregation function in GraphSAGE. GraphSAGE owes its inductivity to its aggregator functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean aggregator \n",
    "Mean aggregator is as simple as you thought it would be. In mean aggregator we simply\n",
    "take the elementwise mean of the vectors in **{h<sub>u</sub><sup>k-1</sup>     ∀u ∈ N (v)}**.\n",
    "In other words, we can average embeddings of all nodes in the neighborhood to construct the neighborhood embedding.\n",
    "![mean aggregator](img/ma.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pool aggregator\n",
    "Until now, we were using weighted average type of approach. But we could also use pooling type of approach for example we can do elementwise min or max pooling. So this would be another option where we are taking the messages from our neighbours, transforming them and applying some type of pooling technique(max pooling or min pooling).\n",
    "![pool aggregator](img/pa.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above equation max denotes the elementwise max operator and σ is a nonlinear activation function (yes you are right it can be ReLU..). Please note that the function applied before the max pooling can be an arbitrarily deep multi-layer perceptron, but in the original paper simple single-layer architectures is preferred."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM aggregator\n",
    "We could also use a deep neural network like LSTM to learn how to aggregate the neighbours. Since LSTM is not order invariant but we want our aggregator to be order invariant so when we train we could try to train this over several random orderings or permutation to make sure that this will learn that order is not important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
